{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This is implementation of transformer from the original paper [Attention Is All You Need](https://arxiv.org/abs/1706.03762). The task in this implementation is English-Dutch translation.","metadata":{}},{"cell_type":"markdown","source":"**Input embedding**","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport math","metadata":{"execution":{"iopub.status.busy":"2023-08-17T13:29:58.575063Z","iopub.execute_input":"2023-08-17T13:29:58.575974Z","iopub.status.idle":"2023-08-17T13:29:58.581191Z","shell.execute_reply.started":"2023-08-17T13:29:58.575925Z","shell.execute_reply":"2023-08-17T13:29:58.580075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class InputEmbedding(nn.Module):\n    \n    def __init__(self, d_model: int, vocab_size: int):\n        super().__init__()\n        self.d_model = d_model  # In the paper they use vectors of size d_model=512\n        self.vocab_size = vocab_size\n        self.embedding = nn.Embedding(vocab_size, d_model)\n        \n    def forward(self, x):\n        return self.embedding(x) * math.sqrt(self.d_model)","metadata":{"execution":{"iopub.status.busy":"2023-08-17T13:29:58.582718Z","iopub.execute_input":"2023-08-17T13:29:58.583084Z","iopub.status.idle":"2023-08-17T13:29:58.608141Z","shell.execute_reply.started":"2023-08-17T13:29:58.583052Z","shell.execute_reply":"2023-08-17T13:29:58.607062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Positional encoiding**","metadata":{}},{"cell_type":"code","source":"class PositionalEncoding(nn.Module):\n    \n    def __init__(self, d_model : int, seq_len : int, dropout_probability : float):\n        super().__init__()\n        self.d_model = d_model\n        self.seq_len = seq_len\n        # To avoid overfitting\n        self.dropout = nn.Dropout(dropout_probability)\n        \n        pos_encoding = torch.zeros(seq_len, d_model) # torch.Size([seq_len, d_model])\n        positions = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1) # torch.Size([seq_len, 1])\n        division_term = torch.exp(torch.arange(0, d_model, 2).float() * \n                                  (-math.log(10000.0) / d_model)) # torch.Size([d_model, 1])\n        \n        # Sine for even indices\n        pos_encoding[:, 0::2] = torch.sin(positions * division_term)\n        # Cosine for odd indices\n        pos_encoding[:, 1::2] = torch.cos(positions * division_term)\n        # Add batch dimension\n        pos_encoding = pos_encoding.unsqueeze(0)\n        # Save positional encoiding like parameters\n        self.register_buffer('pos_encoding', pos_encoding)\n        \n    def forward(self, x):\n        # We don't want to spend time learning pos_encoding since it is fixed, so requires_grad_(False)\n        # torch.Size([batch, seq_len, d_model])\n        x = x + (self.pos_encoding[:, :x.shape[1], :]).requires_grad_(False)\n        return self.dropout(x)\n        ","metadata":{"execution":{"iopub.status.busy":"2023-08-17T13:29:58.610220Z","iopub.execute_input":"2023-08-17T13:29:58.610939Z","iopub.status.idle":"2023-08-17T13:29:58.623236Z","shell.execute_reply.started":"2023-08-17T13:29:58.610898Z","shell.execute_reply":"2023-08-17T13:29:58.622234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Layer normalization**","metadata":{}},{"cell_type":"code","source":"class LayerNormalization(nn.Module):\n    \n    def __init__(self, eps : float = 10**-6):\n        super().__init__()\n        # eps is to prevent dividing by zero or when std is very small\n        self.eps = eps\n        # Adding\n        self.bias = nn.Parameter(torch.zeros(1))\n        # Multiplying\n        self.alpha = nn.Parameter(torch.ones(1))\n        \n    def forward(self, x):\n        # x : torch.Size([batch, seq_len, hidden_size])\n        mean = x.mean(dim=-1, keepdim=True) # torch.Size([batch, seq_len, 1])\n        std = x.std(dim=-1, keepdim=True) # torch.Size([batch, seq_len, 1])\n        return self.alpha * (x - mean) / (std + self.eps) + self.bias","metadata":{"execution":{"iopub.status.busy":"2023-08-17T13:29:58.624749Z","iopub.execute_input":"2023-08-17T13:29:58.625339Z","iopub.status.idle":"2023-08-17T13:29:58.633561Z","shell.execute_reply.started":"2023-08-17T13:29:58.625306Z","shell.execute_reply":"2023-08-17T13:29:58.632515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Feed forward**","metadata":{}},{"cell_type":"code","source":"class FeedForward(nn.Module):\n    \n    def __init__(self, d_model : int, d_inner_later : int, dropout_prob : float):\n        super().__init__()\n        self.dropout = nn.Dropout(dropout_prob)\n        # d_inner_later = 2048 in paper\n        self.lin_layer_1 = nn.Linear(d_model, d_inner_later)\n        self.lin_layer_2 = nn.Linear(d_inner_later, d_model)\n        \n    def forward(self, x):\n        # x : (batch, seq_len, d_model)\n        x = self.lin_layer_1(x)\n        # x : (batch, seq_len, d_inner_later)\n        x = torch.relu(x)\n        x = self.dropout(x)\n        # x : (batch, seq_len, d_model)\n        x = self.lin_layer_2(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-08-17T13:29:58.636023Z","iopub.execute_input":"2023-08-17T13:29:58.636663Z","iopub.status.idle":"2023-08-17T13:29:58.647073Z","shell.execute_reply.started":"2023-08-17T13:29:58.636630Z","shell.execute_reply":"2023-08-17T13:29:58.646130Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Multi-Head Attention**","metadata":{}},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n    \n    def __init__(self, d_model : int, num_heads : int, dropout_prob : float):\n        super().__init__()\n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.dropout = nn.Dropout(dropout_prob)\n\n        assert d_model % num_heads == 0\n        \n        self.d_k = d_model // num_heads # Dimension of vector in each head\n        self.W_q = nn.Linear(d_model, d_model)\n        self.W_k = nn.Linear(d_model, d_model)\n        self.W_v = nn.Linear(d_model, d_model)\n        self.W_o = nn.Linear(d_model, d_model)\n        \n    @staticmethod\n    def attention(Q_k, K_k, V_k, mask=None, dropout: nn.Dropout = None):\n        d_k = Q_k.shape[-1]\n        # shape : (batch, h, seq_len, d_k) -> (batch, h, seq_len, seq_len)\n        attention_scores = (Q_k @ K_k.transpose(-2, -1)) / math.sqrt(d_k)\n        if mask is not None:\n            attention_scores.masked_fill_(mask == 0, -1e9)\n        attention_scores = attention_scores.softmax(dim=-1)\n        if dropout is not None:\n            attention_scores = dropout(attention_scores)\n        # shape : (batch, h, seq_len, seq_len) --> (batch, h, seq_len, d_k)\n        return attention_scores @ V_k, attention_scores\n    \n    def forward(self, Q, K, V, mask=None):\n        # shape: (batch, seq_len, d_model) -> (batch, seq_len, d_model)\n        query = self.W_q(Q)\n        key = self.W_k(K)\n        value = self.W_v(V)\n        \n        # shape: (batch, seq_len, d_model) \n        # -> (batch, seq_len, num_heads, d_k) \n        # -> (batch, num_heads, seq_len, d_k)\n        query = query.view(query.shape[0], query.shape[1], self.num_heads, self.d_k).transpose(1, 2)\n        key = key.view(key.shape[0], key.shape[1], self.num_heads, self.d_k).transpose(1, 2)\n        value = value.view(value.shape[0], value.shape[1], self.num_heads, self.d_k).transpose(1, 2)\n        \n        x, self.attention_scores = MultiHeadAttention.attention(query, key, value, mask, self.dropout)\n        \n        # Concat\n        # shape : (batch, num_heads, seq_len, d_k) \n        # -> (batch, seq_len, num_heads, d_k) \n        # -> (batch, seq_len, d_model)\n        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.d_model)\n        \n        # shape : (batch, seq_len, d_model) -> (batch, seq_len, d_model)  \n        return self.W_o(x)","metadata":{"execution":{"iopub.status.busy":"2023-08-17T13:29:58.648678Z","iopub.execute_input":"2023-08-17T13:29:58.649558Z","iopub.status.idle":"2023-08-17T13:29:58.664226Z","shell.execute_reply.started":"2023-08-17T13:29:58.649519Z","shell.execute_reply":"2023-08-17T13:29:58.663232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Residuals**","metadata":{}},{"cell_type":"code","source":"class Residual(nn.Module):\n    \n    def __init__(self, dropout_prob : float):\n        super().__init__()\n        self.dropout = nn.Dropout(dropout_prob)\n        self.layer_norm = LayerNormalization()\n        \n    def forward(self, x, sublayer):\n        return x + self.dropout(self.layer_norm(sublayer(x)))","metadata":{"execution":{"iopub.status.busy":"2023-08-17T13:29:58.676901Z","iopub.execute_input":"2023-08-17T13:29:58.677741Z","iopub.status.idle":"2023-08-17T13:29:58.684213Z","shell.execute_reply.started":"2023-08-17T13:29:58.677709Z","shell.execute_reply":"2023-08-17T13:29:58.683306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Encoder Block**","metadata":{}},{"cell_type":"code","source":"# There is N encoder blocks\nclass EncoderBlock(nn.Module):\n    \n    def __init__(self, self_attention_block: MultiHeadAttention, \n                 feed_forward_block: FeedForward, dropout_prob : float):\n        super().__init__()\n        self.self_attention_block = self_attention_block\n        self.feed_forward_block = feed_forward_block\n        self.residuals = nn.ModuleList([Residual(dropout_prob) for _ in range(2)])\n    \n    def forward(self, x, src_mask):\n        x = self.residuals[0](x, lambda x: self.self_attention_block(x, x, x, src_mask))\n        x = self.residuals[1](x, lambda x: self.feed_forward_block(x))\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-08-17T13:29:58.686367Z","iopub.execute_input":"2023-08-17T13:29:58.686944Z","iopub.status.idle":"2023-08-17T13:29:58.695904Z","shell.execute_reply.started":"2023-08-17T13:29:58.686911Z","shell.execute_reply":"2023-08-17T13:29:58.694696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Encoder**","metadata":{}},{"cell_type":"code","source":"class Encoder(nn.Module):\n    \n    def __init__(self, layers : nn.ModuleList):\n        super().__init__()\n        self.layers = layers\n    \n    def forward(self, x, mask):\n        for layer in self.layers:\n            x = layer(x, mask)\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-08-17T13:29:58.697349Z","iopub.execute_input":"2023-08-17T13:29:58.698072Z","iopub.status.idle":"2023-08-17T13:29:58.708063Z","shell.execute_reply.started":"2023-08-17T13:29:58.698039Z","shell.execute_reply":"2023-08-17T13:29:58.707157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Decoder Block**","metadata":{}},{"cell_type":"code","source":"class DecoderBlock(nn.Module):\n    \n    def __init__(self, self_attention_block: MultiHeadAttention, \n                 cross_attention_block: MultiHeadAttention, \n                 feed_forward_block: FeedForward, dropout_prob : float):\n        super().__init__()\n        self.self_attention_block = self_attention_block\n        self.cross_attention_block = cross_attention_block\n        self.feed_forward_block = feed_forward_block\n        self.residuals = nn.ModuleList([Residual(dropout_prob) for _ in range(3)])\n        \n    def forward(self, x, encoder_output, src_mask, tgt_mask):\n        x = self.residuals[0](x, lambda x: self.self_attention_block(x, x, x, tgt_mask))\n        x = self.residuals[1](x, lambda x: self.cross_attention_block(x, encoder_output, \n                                                                      encoder_output, src_mask))\n        x = self.residuals[2](x, lambda x: self.feed_forward_block(x))\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-08-17T13:29:58.710577Z","iopub.execute_input":"2023-08-17T13:29:58.711614Z","iopub.status.idle":"2023-08-17T13:29:58.719715Z","shell.execute_reply.started":"2023-08-17T13:29:58.711581Z","shell.execute_reply":"2023-08-17T13:29:58.719027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Decoder**","metadata":{}},{"cell_type":"code","source":"class Decoder(nn.Module):\n    \n    def __init__(self, layers : nn.ModuleList):\n        super().__init__()\n        self.layers = layers\n        \n    def forward(self, x, encoder_output, src_mask, tgt_mask):\n        for layer in self.layers:\n            x = layer(x, encoder_output, src_mask, tgt_mask)\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-08-17T13:29:58.722736Z","iopub.execute_input":"2023-08-17T13:29:58.723412Z","iopub.status.idle":"2023-08-17T13:29:58.730273Z","shell.execute_reply.started":"2023-08-17T13:29:58.723386Z","shell.execute_reply":"2023-08-17T13:29:58.729240Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Linear Classifier**","metadata":{}},{"cell_type":"code","source":"class LinearClassifier(nn.Module):\n    \n    def __init__(self, d_model : int, vocab_size : int):\n        super().__init__()\n        self.lin_layer = nn.Linear(d_model, vocab_size)\n        \n    def forward(self, x):\n        x = self.lin_layer(x)\n        x = torch.nn.functional.softmax(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-08-17T13:29:58.731981Z","iopub.execute_input":"2023-08-17T13:29:58.732656Z","iopub.status.idle":"2023-08-17T13:29:58.742340Z","shell.execute_reply.started":"2023-08-17T13:29:58.732621Z","shell.execute_reply":"2023-08-17T13:29:58.741699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Transformer**","metadata":{}},{"cell_type":"code","source":"class Transformer(nn.Module):\n    \n    def __init__(self, encoder: Encoder, decoder: Decoder, \n                 src_embedding: InputEmbedding, tgt_embedding: InputEmbedding,\n                 src_pos: PositionalEncoding, tgt_pos: PositionalEncoding, \n                 lin_classifier: LinearClassifier):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.src_embedding = src_embedding\n        self.tgt_embedding = tgt_embedding\n        self.src_pos = src_pos\n        self.tgt_pos = tgt_pos\n        self.lin_classifier = lin_classifier\n    \n    def encode(self, src, src_mask):\n        src = self.src_embedding(src)\n        src = self.src_pos(src)\n        src = self.encoder(src, src_mask)\n        return src\n    \n    def decode(self, tgt, encoder_output, tgt_mask, src_mask):\n        tgt = self.tgt_embedding(tgt)\n        tgt = self.tgt_pos(tgt)\n        tgt = self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n        return tgt\n    \n    def project(self, x):\n        return self.lin_classifier(x)","metadata":{"execution":{"iopub.status.busy":"2023-08-17T13:29:58.743745Z","iopub.execute_input":"2023-08-17T13:29:58.744441Z","iopub.status.idle":"2023-08-17T13:29:58.754110Z","shell.execute_reply.started":"2023-08-17T13:29:58.744408Z","shell.execute_reply":"2023-08-17T13:29:58.753279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_transformer(src_vocab_size: int, tgt_vocab_size: int, src_seq_len: int, tgt_seq_len: int,\n                      d_model: int = 512, n_layers: int =  6, num_heads: int = 8,\n                      dropout_prob: float = 0.1, d_inner_later_ff: int = 2048) -> Transformer:\n    src_embedding = InputEmbedding(d_model, src_vocab_size)\n    tgt_embedding = InputEmbedding(d_model, tgt_vocab_size)\n    \n    src_pos = PositionalEncoding(d_model, src_seq_len, dropout_prob)\n    tgt_pos = PositionalEncoding(d_model, tgt_seq_len, dropout_prob)\n    \n    encoder_self_attention_block = MultiHeadAttention(d_model, num_heads, dropout_prob)\n    encoder_feed_forward_block = FeedForward(d_model, d_inner_later_ff, dropout_prob)\n    encoder_layers = nn.ModuleList([EncoderBlock(\n        encoder_self_attention_block, encoder_feed_forward_block, dropout_prob) for _ in range(n_layers)])\n    \n    decoder_self_attention_block = MultiHeadAttention(d_model, num_heads, dropout_prob)\n    decoder_cross_attention_block = MultiHeadAttention(d_model, num_heads, dropout_prob)\n    decoder_feed_forward_block = FeedForward(d_model, d_inner_later_ff, dropout_prob)\n    decoder_layers = nn.ModuleList([DecoderBlock(\n        decoder_self_attention_block, decoder_cross_attention_block, \n        decoder_feed_forward_block, dropout_prob) for _ in range(n_layers)])\n    \n    encoder = Encoder(encoder_layers)\n    decoder = Decoder(decoder_layers)\n    \n    lin_classifier = LinearClassifier(d_model, tgt_vocab_size)\n\n    t = Transformer(encoder, decoder, src_embedding, tgt_embedding, src_pos, tgt_pos, lin_classifier)\n    \n    # Initialize starting parameters\n    for p in t.parameters():\n        if p.dim() > 1:\n            nn.init.xavier_uniform_(p)\n    \n    return t\n\ndef get_model(config, vocab_src_len, vocab_tgt_len):\n    model = build_transformer(vocab_src_len, vocab_tgt_len, \n                              config[\"seq_len\"], config['seq_len'], d_model=config['d_model'])\n    return model","metadata":{"execution":{"iopub.status.busy":"2023-08-17T13:29:58.755667Z","iopub.execute_input":"2023-08-17T13:29:58.756243Z","iopub.status.idle":"2023-08-17T13:29:58.767701Z","shell.execute_reply.started":"2023-08-17T13:29:58.756210Z","shell.execute_reply":"2023-08-17T13:29:58.766967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Tokenize**","metadata":{}},{"cell_type":"code","source":"import os\nfrom torch.utils.data import Dataset, DataLoader, random_split\n\n# Huggingface datasets and tokenizers\nfrom datasets import load_dataset\nfrom tokenizers import Tokenizer\nfrom tokenizers.models import WordLevel\nfrom tokenizers.trainers import WordLevelTrainer\nfrom tokenizers.pre_tokenizers import Whitespace","metadata":{"execution":{"iopub.status.busy":"2023-08-17T13:29:58.769000Z","iopub.execute_input":"2023-08-17T13:29:58.769587Z","iopub.status.idle":"2023-08-17T13:29:58.780653Z","shell.execute_reply.started":"2023-08-17T13:29:58.769554Z","shell.execute_reply":"2023-08-17T13:29:58.779722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_all_sentences(ds, lang):\n    for item in ds:\n        yield item['translation'][lang]\n\ndef get_or_build_tokenizer(config, ds, lang):\n    tokenizer_path = config['tokenizer_file'].format(lang)\n    if not os.path.exists(tokenizer_path):\n        # Most code taken from: https://huggingface.co/docs/tokenizers/quicktour\n        tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n        tokenizer.pre_tokenizer = Whitespace()\n        trainer = WordLevelTrainer(special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"], min_frequency=2)\n        tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer=trainer)\n        tokenizer.save(tokenizer_path)\n    else:\n        tokenizer = Tokenizer.from_file(tokenizer_path)\n    return tokenizer","metadata":{"execution":{"iopub.status.busy":"2023-08-17T13:29:58.782792Z","iopub.execute_input":"2023-08-17T13:29:58.783762Z","iopub.status.idle":"2023-08-17T13:29:58.794591Z","shell.execute_reply.started":"2023-08-17T13:29:58.783718Z","shell.execute_reply":"2023-08-17T13:29:58.793725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Dataset**","metadata":{}},{"cell_type":"code","source":"def causal_mask(size):\n    mask = torch.triu(torch.ones((1, size, size)), diagonal=1).type(torch.int)\n    return mask == 0\n\nclass BilingualDataset(Dataset):\n\n    def __init__(self, ds, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang, seq_len):\n        super().__init__()\n        self.seq_len = seq_len\n\n        self.ds = ds\n        self.tokenizer_src = tokenizer_src\n        self.tokenizer_tgt = tokenizer_tgt\n        self.src_lang = src_lang\n        self.tgt_lang = tgt_lang\n        \n        # Tokens\n        self.sos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[SOS]\")], dtype=torch.int64)\n        self.eos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[EOS]\")], dtype=torch.int64)\n        self.pad_token = torch.tensor([tokenizer_tgt.token_to_id(\"[PAD]\")], dtype=torch.int64)\n        \n    def __len__(self):\n        return len(self.ds)\n    \n    def __getitem__(self, idx):\n        src_data = self.ds[idx]\n        src_text = src_data['translation'][self.src_lang]\n        tgt_text = src_data['translation'][self.tgt_lang]\n        \n        # Split sentence and then give every word its id number\n        # input_tokens : list(int)\n        enc_input_tokens = self.tokenizer_src.encode(src_text).ids\n        dec_input_tokens = self.tokenizer_tgt.encode(tgt_text).ids\n        \n        # Add tokens\n        # Here -2 is for [SOS] and [EOS]\n        enc_num_padding_tokens = self.seq_len - len(enc_input_tokens) - 2\n        # Here -1 is for [SOS]\n        dec_num_padding_tokens = self.seq_len - len(dec_input_tokens) - 1\n        \n        # Make sure the number of padding tokens is not negative. If it is, the sentence is too long\n        if enc_num_padding_tokens < 0 or dec_num_padding_tokens < 0:\n            raise ValueError(\"Sentence too long\")\n        \n        # Concat all the tokens\n        encoder_input = torch.cat([\n                self.sos_token,\n                torch.tensor(enc_input_tokens, dtype=torch.int64),\n                self.eos_token,\n                torch.tensor([self.pad_token] * enc_num_padding_tokens, dtype=torch.int64)], dim=0)\n        \n        decoder_input = torch.cat([\n                self.sos_token,\n                torch.tensor(dec_input_tokens, dtype=torch.int64), \n                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64)], dim=0)\n        \n        target = torch.cat([\n                torch.tensor(dec_input_tokens, dtype=torch.int64),\n                self.eos_token,\n                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64)], dim=0)\n        \n        # size : (1, 1, seq_len)\n        # .unsqueeze(0).unsqueeze(0) -> add seq_len dim and add batch dim\n        encoder_mask = (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int()\n        # size: (1, 1, seq_len) & (1, seq_len, seq_len)\n        decoder_mask = (decoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int() & causal_mask(decoder_input.size(0))\n        \n        return {\n            \"src_text\": src_text,\n            \"tgt_text\": tgt_text,\n            \"encoder_input\": encoder_input,  # (seq_len)\n            \"decoder_input\": decoder_input,  # (seq_len)\n            \"target\": target,  # (seq_len)\n            \"encoder_mask\": encoder_mask, # (1, 1, seq_len)\n            \"decoder_mask\": decoder_mask, # (1, 1, seq_len) & (1, seq_len, seq_len),\n        }","metadata":{"execution":{"iopub.status.busy":"2023-08-17T13:29:58.796539Z","iopub.execute_input":"2023-08-17T13:29:58.796787Z","iopub.status.idle":"2023-08-17T13:29:58.814156Z","shell.execute_reply.started":"2023-08-17T13:29:58.796764Z","shell.execute_reply":"2023-08-17T13:29:58.813235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_ds(config):\n    ds_raw = load_dataset('opus_books', f\"{config['lang_src']}-{config['lang_tgt']}\", split='train')\n    \n    if config[\"limit_ds\"] is not None:\n        ds_raw = list(ds_raw)[:config[\"limit_ds\"]]\n    \n    # Build tokenizers\n    tokenizer_src = get_or_build_tokenizer(config, ds_raw, config['lang_src'])\n    tokenizer_tgt = get_or_build_tokenizer(config, ds_raw, config['lang_tgt'])\n\n    # Keep 90% for training, 10% for validation\n    train_ds_size = int(0.9 * len(ds_raw))\n    val_ds_size = len(ds_raw) - train_ds_size\n    train_ds_raw, val_ds_raw = random_split(ds_raw, [train_ds_size, val_ds_size])\n\n    train_ds = BilingualDataset(train_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n    val_ds = BilingualDataset(val_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n    \n    # Find the maximum length of each sentence in the source and target sentence\n    max_len_src = 0\n    max_len_tgt = 0\n    for item in ds_raw:\n        src_ids = tokenizer_src.encode(item['translation'][config['lang_src']]).ids\n        tgt_ids = tokenizer_tgt.encode(item['translation'][config['lang_tgt']]).ids\n        max_len_src = max(max_len_src, len(src_ids))\n        max_len_tgt = max(max_len_tgt, len(tgt_ids))\n    print(f'Max length of source sentence: {max_len_src}')\n    print(f'Max length of target sentence: {max_len_tgt}')\n    \n\n    train_dataloader = DataLoader(train_ds, batch_size=config['batch_size'], shuffle=True)\n    val_dataloader = DataLoader(val_ds, batch_size=1, shuffle=True)\n\n    return train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt","metadata":{"execution":{"iopub.status.busy":"2023-08-17T13:29:58.817534Z","iopub.execute_input":"2023-08-17T13:29:58.817825Z","iopub.status.idle":"2023-08-17T13:29:58.830240Z","shell.execute_reply.started":"2023-08-17T13:29:58.817799Z","shell.execute_reply":"2023-08-17T13:29:58.829305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Configuration**","metadata":{}},{"cell_type":"code","source":"config = {\n    \"batch_size\": 8,\n    \"num_epochs\": 2,\n    \"lr\": 10**-4,\n    \"seq_len\": 750,\n    \"d_model\": 512,\n    \"lang_src\": \"en\",\n    \"lang_tgt\": \"nl\",\n    \"model_folder\": \"weights\",\n    \"model_basename\": \"my_first_run\",\n    \"tokenizer_file\": \"tokenizer_{0}.json\",\n    \"experiment_name\": \"runs/tmodel\",\n    \"limit_ds\": 300, # if it is None then it is not limited and uses the whole ds\n    \"validation_num_examples\": 2\n}\n\ndef get_weights_file_path(config):\n    model_folder = config[\"model_folder\"]\n    model_basename = config[\"model_basename\"]\n    model_filename = f\"{model_basename}.pt\"\n    return os.path.join(\"/kaggle\", \"working\", model_folder, model_filename)","metadata":{"execution":{"iopub.status.busy":"2023-08-17T13:29:58.834186Z","iopub.execute_input":"2023-08-17T13:29:58.834470Z","iopub.status.idle":"2023-08-17T13:29:58.843596Z","shell.execute_reply.started":"2023-08-17T13:29:58.834446Z","shell.execute_reply":"2023-08-17T13:29:58.842587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Validation**","metadata":{}},{"cell_type":"code","source":"def decode_completely(config, model, encoder_input, encoder_mask, tokenizer_src, tokenizer_tgt, device):\n    sos_idx = tokenizer_src.token_to_id('[SOS]')\n    eos_idx = tokenizer_src.token_to_id('[EOS]')\n    \n    encoder_output = model.encode(encoder_input, encoder_mask)\n    # Initialize the decoder input with the sos token\n    decoder_input = torch.empty(1, 1).fill_(sos_idx).type_as(encoder_input).to(device)\n    \n    while True:\n        if decoder_input.size(1) >= config[\"seq_len\"]:\n            break\n        \n        # Make a mask\n        decoder_mask = causal_mask(decoder_input.size(1)).type_as(encoder_mask).to(device)\n        \n        # calculate output\n        model_out = model.decode(decoder_input, encoder_output, decoder_mask, encoder_mask)\n        proj = model.project(model_out[:, -1])\n        prob, next_word_idx = torch.max(proj, dim=1)\n\n        decoder_input = torch.cat([\n            decoder_input, \n            torch.empty(1, 1).type_as(encoder_input).fill_(next_word_idx.item()).to(device)], \n            dim=1)\n\n        if next_word_idx == eos_idx:\n            break\n            \n    return decoder_input.squeeze(0)\n    \n\ndef validate(config, model, val_dataloader, tokenizer_src, tokenizer_tgt, device):\n    model.eval()\n    count = 0\n    \n#     source_texts = []\n#     target_texts = []\n#     predicted_texts = []\n    \n    with torch.no_grad():\n        for batch in val_dataloader:\n            count += 1\n            \n            encoder_input = batch['encoder_input'].to(device) # (B, seq_len)\n            encoder_mask = batch['encoder_mask'].to(device) # (B, 1, 1, seq_len)\n            \n            model_out = decode_completely(config, model, encoder_input, encoder_mask, \n                                          tokenizer_src, tokenizer_tgt, device)\n            \n            source_text = batch[\"src_text\"][0]\n            target_text = batch[\"tgt_text\"][0]\n            predicted_text = tokenizer_tgt.decode(model_out.detach().cpu().numpy())\n            \n#             source_texts.append(source_text)\n#             target_texts.append(target_text)\n#             predicted_texts.append(predicted_text)\n            \n            # Log results\n            print(\"-----------------------------\")\n            print(\"SOURCE:\", source_text)\n            print(\"TARGET:\", target_text)\n            print(\"PREDICTED:\", predicted_text)\n            \n            if count == config[\"validation_num_examples\"]:\n                break","metadata":{"execution":{"iopub.status.busy":"2023-08-17T13:29:58.845089Z","iopub.execute_input":"2023-08-17T13:29:58.846102Z","iopub.status.idle":"2023-08-17T13:29:58.859589Z","shell.execute_reply.started":"2023-08-17T13:29:58.846069Z","shell.execute_reply":"2023-08-17T13:29:58.858905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Train**","metadata":{}},{"cell_type":"code","source":"from torch.utils.tensorboard import SummaryWriter\nfrom tqdm import tqdm\n\ndef train_model(config):\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(\"Using device:\", device)\n    \n    if not os.path.exists(config[\"model_folder\"]):\n        os.makedirs(config[\"model_folder\"])\n        print(\"Created:\", config[\"model_folder\"])\n    else:\n        print(config[\"model_folder\"])\n    \n    train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)\n    model = get_model(config, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'])\n    loss_fn = torch.nn.CrossEntropyLoss(ignore_index=tokenizer_src.token_to_id(\"[PAD]\")).to(device)\n    # TensorBoard\n    writer = SummaryWriter(config['experiment_name'])\n    \n    for epoch in tqdm(range(config[\"num_epochs\"])):\n        for batch in tqdm(train_dataloader):\n            model.train()\n            \n            encoder_input = batch['encoder_input'].to(device) # (B, seq_len)\n            decoder_input = batch['decoder_input'].to(device) # (B, seq_len)\n            encoder_mask = batch['encoder_mask'].to(device) # (B, 1, 1, seq_len)\n            decoder_mask = batch['decoder_mask'].to(device) # (B, 1, seq_len, seq_len)\n            \n            # size : (batch, seq_len, d_model)\n            encoder_output = model.encode(encoder_input, encoder_mask)\n            decoder_output = model.decode(decoder_input, encoder_output, decoder_mask, encoder_mask)\n            # size : (batch, seq_len, tgt_vocab_size)\n            proj_output = model.project(decoder_output)\n            \n            # size : (batch, seq_len)\n            target = batch['target'].to(device)\n            \n            loss = loss_fn(proj_output.view(-1, tokenizer_tgt.get_vocab_size()), target.view(-1))\n            \n            # Log the loss\n            writer.add_scalar('train loss', loss.item(), epoch)\n            writer.flush()\n            \n            # Backpropagation\n            loss.backward()\n            \n            # Update the weights\n            optimizer.step()\n            optimizer.zero_grad(set_to_none=True)\n            \n        # Validate\n        validate(config, model, val_dataloader, tokenizer_src, tokenizer_tgt, device)\n        \n        # Save the model after every epoch\n        model_filename = get_weights_file_path(config)\n        torch.save({\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict()\n        }, model_filename)","metadata":{"execution":{"iopub.status.busy":"2023-08-17T13:29:58.861021Z","iopub.execute_input":"2023-08-17T13:29:58.861829Z","iopub.status.idle":"2023-08-17T13:29:58.876249Z","shell.execute_reply.started":"2023-08-17T13:29:58.861775Z","shell.execute_reply":"2023-08-17T13:29:58.875402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_model(config)","metadata":{"execution":{"iopub.status.busy":"2023-08-17T13:29:58.877586Z","iopub.execute_input":"2023-08-17T13:29:58.878352Z","iopub.status.idle":"2023-08-17T13:31:52.395026Z","shell.execute_reply.started":"2023-08-17T13:29:58.878318Z","shell.execute_reply":"2023-08-17T13:31:52.393971Z"},"trusted":true},"execution_count":null,"outputs":[]}]}